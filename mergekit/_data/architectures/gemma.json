{
  "model_type": "gemma",
  "architectures": "GemmaForCausalLM",
  "pre_weights": [
    "model.embed_tokens.weight",
    "model.layers.*.input_layernorm.weight",
    "model.layers.*.mlp.down_proj.weight",
    "model.layers.*.mlp.gate_proj.weight",
    "model.layers.*.mlp.up_proj.weight",
    "model.layers.*.post_attention_layernorm.weight",
    "model.layers.*.self_attn.k_proj.weight",
    "model.layers.*.self_attn.o_proj.weight",
    "model.layers.*.self_attn.q_proj.weight",
    "model.layers.*.self_attn.v_proj.weight",
    "model.norm.weight"
  ]
}
